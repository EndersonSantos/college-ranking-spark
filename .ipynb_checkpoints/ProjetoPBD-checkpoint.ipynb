{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38fd3425-fc77-4155-b970-63386b247b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Projeto PBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362ecfbb-8033-45df-a055-f033a0223783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e416407-1dd4-468c-8d90-d574e9e9fd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0e9eaf-cc20-4408-9b96-1099e478f7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 20:20:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/06 20:20:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build our own SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"BigData\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",6)\\\n",
    "    .config(\"spark.sql.repl.eagereval.enabled\",True)\\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329a1522-05be-4f3d-8173-f5c070ffdd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pt-olx-w6h0qt09f3.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigData</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x133b17f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69aee9e-b38f-495a-bc9c-e602cd20a4e7",
   "metadata": {},
   "source": [
    "### 1ªInicio\n",
    "* Read CSV\n",
    "* Perform EDA\n",
    "\n",
    "\t- Understand the problem\n",
    "\t- Which one makes sense to keep for Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b471065c-2cd7-443a-86bc-34f016ccb6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time,site_name,posa_continent,user_location_country,user_location_region,user_location_city,orig_destination_distance,user_id,is_mobile,is_package,channel,srch_ci,srch_co,srch_adults_cnt,srch_children_cnt,srch_rm_cnt,srch_destination_id,srch_destination_type_id,is_booking,cnt,hotel_continent,hotel_country,hotel_market,hotel_cluster\n",
      "2014-08-11 07:46:59,2,3,66,348,48862,2234.2641,12,0,1,9,2014-08-27,2014-08-31,2,0,1,8250,1,0,3,2,50,628,1\n",
      "2014-09-18 08:52:42,2,3,66,462,12565,106.4274,1198182,0,0,0,2014-09-18,2014-09-19,1,0,1,18811,1,1,1,2,50,592,42\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 train.csv\n",
    "! tail -n 1 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c9032e-f749-4460-b639-c74c3a16eece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filename = 'train.csv'\n",
    "df_hotels =  spark.read.csv(filename, header=True, inferSchema=\"true\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0c4f9-51b2-4f0a-8c61-b0a3928c4e8d",
   "metadata": {},
   "source": [
    "### Check DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5574fc-bbdb-43bc-b206-dc559b6ab166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- site_name: integer (nullable = true)\n",
      " |-- posa_continent: integer (nullable = true)\n",
      " |-- user_location_country: integer (nullable = true)\n",
      " |-- user_location_region: integer (nullable = true)\n",
      " |-- user_location_city: integer (nullable = true)\n",
      " |-- orig_destination_distance: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- is_mobile: integer (nullable = true)\n",
      " |-- is_package: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- srch_ci: timestamp (nullable = true)\n",
      " |-- srch_co: timestamp (nullable = true)\n",
      " |-- srch_adults_cnt: integer (nullable = true)\n",
      " |-- srch_children_cnt: integer (nullable = true)\n",
      " |-- srch_rm_cnt: integer (nullable = true)\n",
      " |-- srch_destination_id: integer (nullable = true)\n",
      " |-- srch_destination_type_id: integer (nullable = true)\n",
      " |-- is_booking: integer (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      " |-- hotel_continent: integer (nullable = true)\n",
      " |-- hotel_country: integer (nullable = true)\n",
      " |-- hotel_market: integer (nullable = true)\n",
      " |-- hotel_cluster: integer (nullable = true)\n",
      "\n",
      "-RECORD 0----------------------------------------\n",
      " date_time                 | 2014-08-11 07:46:59 \n",
      " site_name                 | 2                   \n",
      " posa_continent            | 3                   \n",
      " user_location_country     | 66                  \n",
      " user_location_region      | 348                 \n",
      " user_location_city        | 48862               \n",
      " orig_destination_distance | 2234.2641           \n",
      " user_id                   | 12                  \n",
      " is_mobile                 | 0                   \n",
      " is_package                | 1                   \n",
      " channel                   | 9                   \n",
      " srch_ci                   | 2014-08-27 00:00:00 \n",
      " srch_co                   | 2014-08-31 00:00:00 \n",
      " srch_adults_cnt           | 2                   \n",
      " srch_children_cnt         | 0                   \n",
      " srch_rm_cnt               | 1                   \n",
      " srch_destination_id       | 8250                \n",
      " srch_destination_type_id  | 1                   \n",
      " is_booking                | 0                   \n",
      " cnt                       | 3                   \n",
      " hotel_continent           | 2                   \n",
      " hotel_country             | 50                  \n",
      " hotel_market              | 628                 \n",
      " hotel_cluster             | 1                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37670293"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hotels.printSchema()\n",
    "df_hotels.show(1, vertical=True)\n",
    "hotels_count = df_hotels.count()\n",
    "hotels_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1471d-6548-4a26-bff3-493d8a88ce80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Evaluate DATA**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b41d5-fbb2-4a9c-9030-a3f1de4aca45",
   "metadata": {},
   "source": [
    "### **Percentagem de Valores NULLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01b3ffd-47da-4f66-ac5e-c5020d544588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nulls in Hotels:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column orig_destination_distance with 13525001 nulls or NaN, out of 37670293 records (35.90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('\\nNulls in Hotels:')\n",
    "cols_to_forget = ['date_time','srch_ci','srch_co']\n",
    "hotels_cols_interest = [x for x in df_hotels.columns if x not in cols_to_forget]\n",
    "for cl in hotels_cols_interest:\n",
    "    k = df_hotels.select(cl).filter(F.col(cl).isNull() | F.isnan(cl)).count()\n",
    "    if k > 0:\n",
    "        print(f'Column {cl} with {k} nulls or NaN, out of {hotels_count} records ({k*100/hotels_count:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507c595-af78-4158-926d-fa21cfba7136",
   "metadata": {},
   "source": [
    "**A variável orig_destination_distanceorig_destination_distance possui 35% dos dados omissos, porém não sabemos se é uma variável importante vamos ver a correlação com a variável target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a208be-aa90-4478-937c-2ad9a7506f49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:=======================================>                (22 + 9) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A correlação entre as variáveis orig_destination_distance e hotel_cluster é: 0.007260029850921165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "correlation = df_hotels.select(corr(\"orig_destination_distance\", \"hotel_cluster\")).collect()[0][0]\n",
    "print(\"A correlação entre as variáveis orig_destination_distance e hotel_cluster é:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cbe065e-fc5c-4b72-8bd7-2b9e71d201ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "hotel_cluster_stats = df_hotels.groupBy(\"hotel_cluster\").agg(corr(\"orig_destination_distance\", \"hotel_cluster\").alias(\"correlation_ratio\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb670b4-64c1-4118-96a1-c8206898eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:===========================================>            (24 + 7) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|hotel_cluster|   correlation_ratio|\n",
      "+-------------+--------------------+\n",
      "|           10|                null|\n",
      "|           18|                null|\n",
      "|           25|                null|\n",
      "|            2|                null|\n",
      "|           46|                null|\n",
      "|            5|                null|\n",
      "|           59|0.001742434346692815|\n",
      "|           73|                null|\n",
      "|           38|                null|\n",
      "|           90|                null|\n",
      "|           50|                null|\n",
      "|           22|                null|\n",
      "|           60|                null|\n",
      "|           97|                null|\n",
      "|           14|                null|\n",
      "|           45|-0.01202740198397...|\n",
      "|           88|                null|\n",
      "|            4|                null|\n",
      "|            1|                null|\n",
      "|           30|                null|\n",
      "|           29|0.003254423942772334|\n",
      "|            6|                null|\n",
      "|           42|                null|\n",
      "|           68|                null|\n",
      "|           75|                null|\n",
      "|           99|                null|\n",
      "|           20|-0.00270968707395...|\n",
      "|           40|                null|\n",
      "|           54|                null|\n",
      "|           39|                null|\n",
      "|           19|                null|\n",
      "|           33|                null|\n",
      "|           34|                null|\n",
      "|           96|                null|\n",
      "|           61|                null|\n",
      "|           70|                null|\n",
      "|           98|                null|\n",
      "|           28|-0.00214995460589...|\n",
      "|           77|                null|\n",
      "|           58|                null|\n",
      "|           36|                null|\n",
      "|           95|                null|\n",
      "|           65|                null|\n",
      "|           56|                null|\n",
      "|           67|                null|\n",
      "|           76|                null|\n",
      "|           32|                null|\n",
      "|           79|                null|\n",
      "|           93|                null|\n",
      "|           12|                null|\n",
      "|           31|                null|\n",
      "|           80|0.003591665025978725|\n",
      "|           21|                null|\n",
      "|           41|                null|\n",
      "|           69|                null|\n",
      "|           94|                null|\n",
      "|           24|                null|\n",
      "|           85|                null|\n",
      "|           53|                null|\n",
      "|            8|                null|\n",
      "|           26|0.004308321713313409|\n",
      "|            9|                null|\n",
      "|           43|                null|\n",
      "|           35|                null|\n",
      "|           64|                null|\n",
      "|           89|                null|\n",
      "|           51|-8.90314580960873...|\n",
      "|           17|                null|\n",
      "|            7|                null|\n",
      "|           47|                null|\n",
      "|            0|                null|\n",
      "|           63|                null|\n",
      "|           44|                null|\n",
      "|            3|                null|\n",
      "|           86|                null|\n",
      "|           66|                null|\n",
      "|           87|                null|\n",
      "|           27| 0.02201260403800159|\n",
      "|           74|                null|\n",
      "|           92|                null|\n",
      "|           57|0.012319372701534293|\n",
      "|           13|                null|\n",
      "|           55|                null|\n",
      "|           37|                null|\n",
      "|           83|                null|\n",
      "|           49|                null|\n",
      "|           52|                null|\n",
      "|           16|                null|\n",
      "|           82|                null|\n",
      "|           62|-0.00114438146880...|\n",
      "|           81|                null|\n",
      "|           91|                null|\n",
      "|           72|                null|\n",
      "|           78|                null|\n",
      "|           11|                null|\n",
      "|           15|                null|\n",
      "|           84|                null|\n",
      "|           48|                null|\n",
      "|           71|                null|\n",
      "|           23|                null|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hotel_cluster_stats.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837748de-7e11-419f-8e58-a35056666f54",
   "metadata": {},
   "source": [
    "**A maior parte dos hoteis nem sequer tem essa feature, por isso vamos excluir a feature toda** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d566d24-2350-4371-96e6-712d10c90a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_cols_interest.remove('orig_destination_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa1e98-0ca7-4eb9-a01f-56acbf51427c",
   "metadata": {},
   "source": [
    "### **Summary to figure out outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b6ff29a-646a-445f-a6b2-05eb332b895f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:======================================================> (30 + 1) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------\n",
      " summary                  | count               \n",
      " site_name                | 37670293            \n",
      " posa_continent           | 37670293            \n",
      " user_location_country    | 37670293            \n",
      " user_location_region     | 37670293            \n",
      " user_location_city       | 37670293            \n",
      " user_id                  | 37670293            \n",
      " is_mobile                | 37670293            \n",
      " is_package               | 37670293            \n",
      " channel                  | 37670293            \n",
      " srch_adults_cnt          | 37670293            \n",
      " srch_children_cnt        | 37670293            \n",
      " srch_rm_cnt              | 37670293            \n",
      " srch_destination_id      | 37670293            \n",
      " srch_destination_type_id | 37670293            \n",
      " is_booking               | 37670293            \n",
      " cnt                      | 37670293            \n",
      " hotel_continent          | 37670293            \n",
      " hotel_country            | 37670293            \n",
      " hotel_market             | 37670293            \n",
      " hotel_cluster            | 37670293            \n",
      "-RECORD 1---------------------------------------\n",
      " summary                  | mean                \n",
      " site_name                | 9.795271329585889   \n",
      " posa_continent           | 2.6804730188851997  \n",
      " user_location_country    | 86.10880194109454   \n",
      " user_location_region     | 308.4060117610447   \n",
      " user_location_city       | 27753.044729330883  \n",
      " user_id                  | 604451.7531778422   \n",
      " is_mobile                | 0.1349265056154461  \n",
      " is_package               | 0.24890422275186444 \n",
      " channel                  | 5.8707613981128315  \n",
      " srch_adults_cnt          | 2.0242958290767743  \n",
      " srch_children_cnt        | 0.3321221579030458  \n",
      " srch_rm_cnt              | 1.1126628083301608  \n",
      " srch_destination_id      | 14441.090543760836  \n",
      " srch_destination_type_id | 2.5822799148389954  \n",
      " is_booking               | 0.07965674702875288 \n",
      " cnt                      | 1.4833839227106622  \n",
      " hotel_continent          | 3.1563047837190967  \n",
      " hotel_country            | 81.29685165974153   \n",
      " hotel_market             | 600.461883638654    \n",
      " hotel_cluster            | 49.80860501934509   \n",
      "-RECORD 2---------------------------------------\n",
      " summary                  | stddev              \n",
      " site_name                | 11.9675435665128    \n",
      " posa_continent           | 0.7480393482506577  \n",
      " user_location_country    | 59.24310334783878   \n",
      " user_location_region     | 208.44374973856722  \n",
      " user_location_city       | 16782.553195680346  \n",
      " user_id                  | 350617.4620408585   \n",
      " is_mobile                | 0.34164505966916764 \n",
      " is_package               | 0.43237820899182056 \n",
      " channel                  | 3.71709455862911    \n",
      " srch_adults_cnt          | 0.9116678125665977  \n",
      " srch_children_cnt        | 0.7314980986397146  \n",
      " srch_rm_cnt              | 0.45911549963856946 \n",
      " srch_destination_id      | 11066.302332627309  \n",
      " srch_destination_type_id | 2.153018959399955   \n",
      " is_booking               | 0.2707610600283715  \n",
      " cnt                      | 1.2197755786558424  \n",
      " hotel_continent          | 1.6231886782105662  \n",
      " hotel_country            | 56.171188062887815  \n",
      " hotel_market             | 511.73912727922396  \n",
      " hotel_cluster            | 28.915950805004293  \n",
      "-RECORD 3---------------------------------------\n",
      " summary                  | min                 \n",
      " site_name                | 2                   \n",
      " posa_continent           | 0                   \n",
      " user_location_country    | 0                   \n",
      " user_location_region     | 0                   \n",
      " user_location_city       | 0                   \n",
      " user_id                  | 0                   \n",
      " is_mobile                | 0                   \n",
      " is_package               | 0                   \n",
      " channel                  | 0                   \n",
      " srch_adults_cnt          | 0                   \n",
      " srch_children_cnt        | 0                   \n",
      " srch_rm_cnt              | 0                   \n",
      " srch_destination_id      | 0                   \n",
      " srch_destination_type_id | 0                   \n",
      " is_booking               | 0                   \n",
      " cnt                      | 1                   \n",
      " hotel_continent          | 0                   \n",
      " hotel_country            | 0                   \n",
      " hotel_market             | 0                   \n",
      " hotel_cluster            | 0                   \n",
      "-RECORD 4---------------------------------------\n",
      " summary                  | max                 \n",
      " site_name                | 53                  \n",
      " posa_continent           | 4                   \n",
      " user_location_country    | 239                 \n",
      " user_location_region     | 1027                \n",
      " user_location_city       | 56508               \n",
      " user_id                  | 1198785             \n",
      " is_mobile                | 1                   \n",
      " is_package               | 1                   \n",
      " channel                  | 10                  \n",
      " srch_adults_cnt          | 9                   \n",
      " srch_children_cnt        | 9                   \n",
      " srch_rm_cnt              | 8                   \n",
      " srch_destination_id      | 65107               \n",
      " srch_destination_type_id | 9                   \n",
      " is_booking               | 1                   \n",
      " cnt                      | 269                 \n",
      " hotel_continent          | 6                   \n",
      " hotel_country            | 212                 \n",
      " hotel_market             | 2117                \n",
      " hotel_cluster            | 99                  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_hotels.describe(hotels_cols_interest).show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3d636-7a86-4ce3-ba7c-ab3f61bc839a",
   "metadata": {},
   "source": [
    "### **UNIQUENESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faabc16b-7463-4417-936e-38d559a82525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness in Hotels:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column site_name with 45 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column posa_continent with 5 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column user_location_country with 237 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column user_location_region with 1008 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column user_location_city with 50447 distinct values, out of 37670293 records (0.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column user_id with 1198786 distinct values, out of 37670293 records (3.18%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column is_mobile with 2 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column is_package with 2 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column channel with 11 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column srch_adults_cnt with 10 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column srch_children_cnt with 10 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column srch_rm_cnt with 9 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column srch_destination_id with 59455 distinct values, out of 37670293 records (0.16%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column srch_destination_type_id with 10 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column is_booking with 2 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column cnt with 104 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column hotel_continent with 7 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column hotel_country with 213 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column hotel_market with 2118 distinct values, out of 37670293 records (0.01%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 192:==========================================>            (24 + 7) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column hotel_cluster with 100 distinct values, out of 37670293 records (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('\\nUniqueness in Hotels:')\n",
    "for cl in hotels_cols_interest:\n",
    "    k = df_hotels.select(cl).distinct().count()\n",
    "    if k > 0:\n",
    "        print(f'Column {cl} with {k} distinct values, out of {hotels_count} records ({k*100/hotels_count:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc31664-266c-4907-ad5d-6124ba034eb1",
   "metadata": {},
   "source": [
    "# **Saving clean data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e68e18c-7d99-459f-84de-be2224ea0ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "376623"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "with_replacement = False\n",
    "fraction = 0.01          \n",
    "small_df_hotels = df_hotels.sample(withReplacement=with_replacement, \n",
    "                                               fraction=fraction, seed=seed)\n",
    "small_df_hotels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17782d9b-9361-468f-a754-b138a2e52c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 20:24:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/04/06 20:24:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:>                                                      (0 + 10) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 20:24:50 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:50 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:=================>                                    (10 + 10) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 20:24:50 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/04/06 20:24:50 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:50 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:======================>                               (13 + 10) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:===============================>                      (18 + 10) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 20:24:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:==================================>                   (20 + 10) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 20:24:57 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:57 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/04/06 20:24:57 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/04/06 20:24:57 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "small_df_hotels.write.mode(\"overwrite\").parquet(\"small-hotels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e0a04-0e68-4966-b8a7-281e2598b689",
   "metadata": {},
   "source": [
    "### **Final Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "690ef7f1-d4d6-4fc8-9076-50085c006bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean = small_df_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f8c1877-fe35-44bc-8256-3eb678487773",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "376623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels_count = df_clean.count()\n",
    "hotels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c49fc883-9883-4f3f-87f6-a2106f4df316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_time',\n",
       " 'site_name',\n",
       " 'posa_continent',\n",
       " 'user_location_country',\n",
       " 'user_location_region',\n",
       " 'user_location_city',\n",
       " 'orig_destination_distance',\n",
       " 'user_id',\n",
       " 'is_mobile',\n",
       " 'is_package',\n",
       " 'channel',\n",
       " 'srch_ci',\n",
       " 'srch_co',\n",
       " 'srch_adults_cnt',\n",
       " 'srch_children_cnt',\n",
       " 'srch_rm_cnt',\n",
       " 'srch_destination_id',\n",
       " 'srch_destination_type_id',\n",
       " 'is_booking',\n",
       " 'cnt',\n",
       " 'hotel_continent',\n",
       " 'hotel_country',\n",
       " 'hotel_market',\n",
       " 'hotel_cluster']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_forget = []#['date_time','srch_ci','srch_co']\n",
    "hotels_cols_interest = [x for x in df_clean.columns if x not in cols_to_forget]\n",
    "hotels_cols_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f36257-3c2f-4c64-ae67-54ff28112443",
   "metadata": {},
   "source": [
    "### **Descriptive statistics about the data to be used in the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d192a5d8-6833-4963-b374-018622b98cba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nulls in Transactions:\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'isnan(date_time)' due to data type mismatch: argument 1 requires (double or float) type, however, 'date_time' is of timestamp type.;\n'Filter (isnull(date_time#17) OR isnan(date_time#17))\n+- Project [date_time#17]\n   +- Sample 0.0, 0.01, false, 5\n      +- Relation [date_time#17,site_name#18,posa_continent#19,user_location_country#20,user_location_region#21,user_location_city#22,orig_destination_distance#23,user_id#24,is_mobile#25,is_package#26,channel#27,srch_ci#28,srch_co#29,srch_adults_cnt#30,srch_children_cnt#31,srch_rm_cnt#32,srch_destination_id#33,srch_destination_type_id#34,is_booking#35,cnt#36,hotel_continent#37,hotel_country#38,hotel_market#39,hotel_cluster#40] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNulls in Transactions:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cl \u001b[38;5;129;01min\u001b[39;00m hotels_cols_interest:\n\u001b[0;32m----> 3\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mdf_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nulls or NaN, out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhotels_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m/\u001b[39mhotels_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2079\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   2077\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 2079\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition should be string or Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'isnan(date_time)' due to data type mismatch: argument 1 requires (double or float) type, however, 'date_time' is of timestamp type.;\n'Filter (isnull(date_time#17) OR isnan(date_time#17))\n+- Project [date_time#17]\n   +- Sample 0.0, 0.01, false, 5\n      +- Relation [date_time#17,site_name#18,posa_continent#19,user_location_country#20,user_location_region#21,user_location_city#22,orig_destination_distance#23,user_id#24,is_mobile#25,is_package#26,channel#27,srch_ci#28,srch_co#29,srch_adults_cnt#30,srch_children_cnt#31,srch_rm_cnt#32,srch_destination_id#33,srch_destination_type_id#34,is_booking#35,cnt#36,hotel_continent#37,hotel_country#38,hotel_market#39,hotel_cluster#40] csv\n"
     ]
    }
   ],
   "source": [
    "print('\\nNulls in Transactions:')\n",
    "for cl in hotels_cols_interest:\n",
    "    k = df_clean.select(cl).filter(F.col(cl).isNull() | F.isnan(cl)).count()\n",
    "    if k > 0:\n",
    "        print(f'Column {cl} with {k} nulls or NaN, out of {hotels_count} records ({k*100/hotels_count:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b84aaa-c795-4442-a3c4-04ae55d3f1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08094341-e2d0-4c25-801d-bbca675d200d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# few distincts\n",
    "cls_1 = ['site_name', 'user_id', 'is_mobile', 'srch_adults_cnt', 'hotel_cluster', 'is_booking']\n",
    "\n",
    "# numeric types but no nulls\n",
    "cls_2 = ['site_name', 'user_id', 'is_mobile', 'srch_adults_cnt', 'hotel_cluster', 'is_booking', 'hotel_country', 'srch_destination_id', 'is_package']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49699482-b2fc-43de-ba6e-934eee982ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\nUniqueness in Transactions:')\n",
    "for cl in cls_1:\n",
    "    df_clean.select(cl).distinct().sort(cl).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba047dc-9fbe-458c-ad24-04358b857412",
   "metadata": {},
   "source": [
    "### **Correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f8485-dc29-4358-86ee-da72c10eb328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotHistogram(df, xcol, huecol=None):\n",
    "    sns.histplot(data=df, x=xcol, hue=huecol, multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e28111-d46d-4dc3-922e-62d54e474907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(df, xcol, ycol):\n",
    "    sns.lineplot(data=df, x=xcol, y=ycol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d394e4e-9dd4-4305-a454-517b02e7e7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotBar(df, xcol, ycol, huecol=None):\n",
    "    sns.barplot(data=df, x=xcol, y=ycol, hue=huecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf73ac-710f-4829-a4be-d2bbc8d1130a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotScatter(df, xcol, ycol, huecol=None):\n",
    "    sns.scatterplot(data=df, x=xcol, y=ycol, hue=huecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19560b-d2aa-4b96-bc95-b6945ed6d4f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotScatterMatrix(df, huecol=None):\n",
    "    sns.pairplot(data=df, hue=huecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d03a3b-6aaf-45c5-b944-f9ecd2881fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotCorrelationMatrix_1(df, annot=False):\n",
    "    # compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "    \n",
    "    # generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    #cmap='coolwarm'\n",
    "\n",
    "    # draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=annot,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c532db6-e273-4696-bd3a-b2b5576440b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotCorrelationMatrix_2(df):\n",
    "    # compute a correlation matrix and convert to long-form\n",
    "    corr_mat = df.corr().stack().reset_index(name=\"correlation\")\n",
    "    # draw each cell as a scatter point with varying size and color\n",
    "    g = sns.relplot(\n",
    "        data=corr_mat,\n",
    "        x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n",
    "        palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n",
    "        height=10, sizes=(50, 250), size_norm=(-.2, .8),\n",
    "    )\n",
    "\n",
    "    # tweak the figure to finalize\n",
    "    g.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n",
    "    g.despine(left=True, bottom=True)\n",
    "    g.ax.margins(.02)\n",
    "    for label in g.ax.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "    for artist in g.legend.legendHandles:\n",
    "        artist.set_edgecolor(\".7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becdda7-1e14-4393-bb37-6074645aeb7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_corr = cls_2\n",
    "\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=cols_corr, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df_clean).select(vector_col)\n",
    "\n",
    "# get correlation matrix - it can be Pearson’s (default) or Spearman’s correlation\n",
    "\n",
    "# corr = Correlation.corr(df_vector, vector_col).head()\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(corr[0]))\n",
    "\n",
    "# corr = Correlation.corr(df_vector, vector_col, \"spearman\").head()\n",
    "# print(\"Spearman correlation matrix:\\n\" + str(corr[0]))\n",
    "\n",
    "corr_matrix = Correlation.corr(df_vector, vector_col).collect()[0][0].toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c5073-3943-4932-afd4-69cb2628b658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame(data = corr_matrix, index=cols_corr, columns=cols_corr)\n",
    "plotCorrelationMatrix_1(df_plot, annot=True)\n",
    "plt.title('Correlations among numerical features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72d9a8-4400-4a55-8b9f-5cbdaae9c199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_plot = ( df_clean\n",
    "                .groupby('hotel_cluster')\n",
    "                .count()\n",
    "                .sort('hotel_cluster', ascending=True)\n",
    "                .toPandas()\n",
    "          )\n",
    "plotBar(df_plot, 'hotel_cluster', 'count')\n",
    "plt.title('Number of transactions by year')\n",
    "plt.show()\n",
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5e65d-1e61-43b5-b470-12cb8bb10add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_plot = ( df_clean\n",
    "            .groupby(['Year', 'Month'])\n",
    "            .count()\n",
    "            .withColumn('Year - Month', F.concat('Year', 'Month'))\n",
    "            #.sort('Year - Month', ascending=True)\n",
    "            .toPandas()\n",
    "          )\n",
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a360f-9aaf-4dfb-966a-00359929b06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1da1e91-168a-449a-ba7f-3a23c206c23f",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3289236-39fe-4db2-a216-f52e86837cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:===================================================>   (29 + 2) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[407.275s][warning][gc,alloc] Executor task launch worker for task 8.0 in stage 208.0 (TID 1667): Retried waiting for GCLocker too often allocating 155194 words\n",
      "[407.275s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 208.0 (TID 1659): Retried waiting for GCLocker too often allocating 146739 words\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_0 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_8 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_0 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_8 could not be removed as it was not found on disk or in memory\n",
      "[407.324s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 208.0 (TID 1660): Retried waiting for GCLocker too often allocating 190547 words\n",
      "23/04/06 20:26:56 ERROR Executor: Exception in task 0.0 in stage 208.0 (TID 1659)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2098)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1675)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:122)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4938/0x000000080025b040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2230/0x0000000801060040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "23/04/06 20:26:56 ERROR Executor: Exception in task 8.0 in stage 208.0 (TID 1667)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2098)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1675)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:122)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4938/0x000000080025b040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2230/0x0000000801060040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_1 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_1 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 ERROR Executor: Exception in task 1.0 in stage 208.0 (TID 1660)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/06 20:26:56 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 208.0 (TID 1660),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/06 20:26:56 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 208.0 (TID 1659),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2098)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1675)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:122)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4938/0x000000080025b040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2230/0x0000000801060040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "23/04/06 20:26:56 WARN TaskSetManager: Lost task 1.0 in stage 208.0 (TID 1660) (pt-olx-w6h0qt09f3.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "23/04/06 20:26:56 ERROR TaskSetManager: Task 1 in stage 208.0 failed 1 times; aborting job\n",
      "23/04/06 20:26:56 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 208.0 (TID 1667),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2098)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1675)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:122)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4938/0x000000080025b040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2230/0x0000000801060040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "23/04/06 20:26:56 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 208.0 failed 1 times, most recent failure: Lost task 1.0 in stage 208.0 (TID 1660) (pt-olx-w6h0qt09f3.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:973)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "[407.473s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 208.0 (TID 1668): Retried waiting for GCLocker too often allocating 131074 words\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_9 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_424_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_7 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_424_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_2 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_424_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_5 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_424_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_4 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_424_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_424_6 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_425_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_425_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_9 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_7 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_425_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_4 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_425_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_6 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Putting block rdd_425_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_5 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 WARN BlockManager: Block rdd_425_2 could not be removed as it was not found on disk or in memory\n",
      "23/04/06 20:26:56 ERROR Executor: Exception in task 9.0 in stage 208.0 (TID 1668)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/06 20:26:56 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 208.0 (TID 1668),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/06 20:26:56 WARN TaskSetManager: Lost task 2.0 in stage 208.0 (TID 1661) (pt-olx-w6h0qt09f3.home executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/06 20:26:56 WARN TaskSetManager: Lost task 7.0 in stage 208.0 (TID 1666) (pt-olx-w6h0qt09f3.home executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/06 20:26:56 WARN TaskSetManager: Lost task 5.0 in stage 208.0 (TID 1664) (pt-olx-w6h0qt09f3.home executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/06 20:26:56 WARN TaskSetManager: Lost task 6.0 in stage 208.0 (TID 1665) (pt-olx-w6h0qt09f3.home executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/06 20:26:56 WARN TaskSetManager: Lost task 4.0 in stage 208.0 (TID 1663) (pt-olx-w6h0qt09f3.home executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o410.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 208.0 failed 1 times, most recent failure: Lost task 1.0 in stage 208.0 (TID 1660) (pt-olx-w6h0qt09f3.home executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:973)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Configurando o modelo ALS (Alternating Least Squares) para fazer as recomendações\u001b[39;00m\n\u001b[1;32m     13\u001b[0m als \u001b[38;5;241m=\u001b[39m ALS(userCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, itemCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotel_cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m, ratingCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_booking\u001b[39m\u001b[38;5;124m\"\u001b[39m, coldStartStrategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fazendo as recomendações para cada usuário com base em seus históricos de reservas e outros atributos\u001b[39;00m\n\u001b[1;32m     17\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrecommendForAllUsers(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o410.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 208.0 failed 1 times, most recent failure: Lost task 1.0 in stage 208.0 (TID 1660) (pt-olx-w6h0qt09f3.home executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:973)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@3be74327 rejected from java.util.concurrent.ThreadPoolExecutor@3fd8c083[Shutting down, pool size = 28, active threads = 1, queued tasks = 0, completed tasks = 720]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@4a150a87 rejected from java.util.concurrent.ThreadPoolExecutor@3fd8c083[Shutting down, pool size = 4, active threads = 1, queued tasks = 0, completed tasks = 720]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/enderson.santos/.pyenv/versions/3.10.9/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/enderson.santos/.pyenv/versions/3.10.9/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/enderson.santos/.pyenv/versions/3.10.9/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Criando um vetor com os atributos que serão usados para fazer as recomendações\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"srch_children_cnt\", \"is_mobile\", \"is_booking\", \"is_package\", \"user_location_country\"], outputCol=\"features\")\n",
    "df_hotels = vectorAssembler.transform(df_hotels)\n",
    "\n",
    "# Separando os dados em conjuntos de treinamento e teste\n",
    "(training, test) = df_hotels.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Configurando o modelo ALS (Alternating Least Squares) para fazer as recomendações\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"hotel_cluster\", ratingCol=\"is_booking\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Fazendo as recomendações para cada usuário com base em seus históricos de reservas e outros atributos\n",
    "recommendations = model.recommendForAllUsers(10)\n",
    "\n",
    "# Mostrando as recomendações para um usuário específico\n",
    "recommendations.filter(recommendations.user_id == 1234).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bf0fb95-53a5-4866-8b3c-a51fffc60ce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recommendations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecommendations\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(recommendations\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1234\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recommendations' is not defined"
     ]
    }
   ],
   "source": [
    "recommendations.filter(recommendations.user_id == 1234).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf569c-a7b4-44e2-9d70-838963239028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3276e-598f-45b8-a7bb-e773b7778890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
