{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38fd3425-fc77-4155-b970-63386b247b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Projeto PBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362ecfbb-8033-45df-a055-f033a0223783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m  \n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# from ydata_profiling import ProfileReport\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e416407-1dd4-468c-8d90-d574e9e9fd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0e9eaf-cc20-4408-9b96-1099e478f7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 19:58:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build our own SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"BigData\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",6)\\\n",
    "    .config(\"spark.sql.repl.eagereval.enabled\",True)\\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329a1522-05be-4f3d-8173-f5c070ffdd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.89.130:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession object at 0x7fd10c0f0fa0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69aee9e-b38f-495a-bc9c-e602cd20a4e7",
   "metadata": {},
   "source": [
    "### 1ÂªInicio\n",
    "* Read CSV\n",
    "* Perform EDA\n",
    "\n",
    "\t- Understand the problem\n",
    "\t- Which one makes sense to keep for Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b471065c-2cd7-443a-86bc-34f016ccb6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time,site_name,posa_continent,user_location_country,user_location_region,user_location_city,orig_destination_distance,user_id,is_mobile,is_package,channel,srch_ci,srch_co,srch_adults_cnt,srch_children_cnt,srch_rm_cnt,srch_destination_id,srch_destination_type_id,is_booking,cnt,hotel_continent,hotel_country,hotel_market,hotel_cluster\n",
      "2014-08-11 07:46:59,2,3,66,348,48862,2234.2641,12,0,1,9,2014-08-27,2014-08-31,2,0,1,8250,1,0,3,2,50,628,1\n",
      "\n",
      "2014-09-18 08:52:42,2,3,66,462,12565,106.4274,1198182,0,0,0,2014-09-18,2014-09-19,1,0,1,18811,1,1,1,2,50,592,42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 train.csv\n",
    "! tail -n 1 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c9032e-f749-4460-b639-c74c3a16eece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'train.csv'\n",
    "df_hotels =  spark.read.csv(filename, header=True, inferSchema=\"true\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0c4f9-51b2-4f0a-8c61-b0a3928c4e8d",
   "metadata": {},
   "source": [
    "### Check DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5574fc-bbdb-43bc-b206-dc559b6ab166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- site_name: integer (nullable = true)\n",
      " |-- posa_continent: integer (nullable = true)\n",
      " |-- user_location_country: integer (nullable = true)\n",
      " |-- user_location_region: integer (nullable = true)\n",
      " |-- user_location_city: integer (nullable = true)\n",
      " |-- orig_destination_distance: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- is_mobile: integer (nullable = true)\n",
      " |-- is_package: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- srch_ci: timestamp (nullable = true)\n",
      " |-- srch_co: timestamp (nullable = true)\n",
      " |-- srch_adults_cnt: integer (nullable = true)\n",
      " |-- srch_children_cnt: integer (nullable = true)\n",
      " |-- srch_rm_cnt: integer (nullable = true)\n",
      " |-- srch_destination_id: integer (nullable = true)\n",
      " |-- srch_destination_type_id: integer (nullable = true)\n",
      " |-- is_booking: integer (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      " |-- hotel_continent: integer (nullable = true)\n",
      " |-- hotel_country: integer (nullable = true)\n",
      " |-- hotel_market: integer (nullable = true)\n",
      " |-- hotel_cluster: integer (nullable = true)\n",
      "\n",
      "-RECORD 0----------------------------------------\n",
      " date_time                 | 2014-08-11 07:46:59 \n",
      " site_name                 | 2                   \n",
      " posa_continent            | 3                   \n",
      " user_location_country     | 66                  \n",
      " user_location_region      | 348                 \n",
      " user_location_city        | 48862               \n",
      " orig_destination_distance | 2234.2641           \n",
      " user_id                   | 12                  \n",
      " is_mobile                 | 0                   \n",
      " is_package                | 1                   \n",
      " channel                   | 9                   \n",
      " srch_ci                   | 2014-08-27 00:00:00 \n",
      " srch_co                   | 2014-08-31 00:00:00 \n",
      " srch_adults_cnt           | 2                   \n",
      " srch_children_cnt         | 0                   \n",
      " srch_rm_cnt               | 1                   \n",
      " srch_destination_id       | 8250                \n",
      " srch_destination_type_id  | 1                   \n",
      " is_booking                | 0                   \n",
      " cnt                       | 3                   \n",
      " hotel_continent           | 2                   \n",
      " hotel_country             | 50                  \n",
      " hotel_market              | 628                 \n",
      " hotel_cluster             | 1                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37670293"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hotels.printSchema()\n",
    "df_hotels.show(1, vertical=True)\n",
    "hotels_count = df_hotels.count()\n",
    "hotels_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1471d-6548-4a26-bff3-493d8a88ce80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Evaluate DATA**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b41d5-fbb2-4a9c-9030-a3f1de4aca45",
   "metadata": {},
   "source": [
    "### **NULLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b3ffd-47da-4f66-ac5e-c5020d544588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nulls in Hotels:\n",
      "Column orig_destination_distance with 13525001 nulls or NaN, out of 37670293 records (35.90%)\n"
     ]
    }
   ],
   "source": [
    "print('\\nNulls in Hotels:')\n",
    "cols_to_forget = ['date_time','srch_ci','srch_co']\n",
    "hotels_cols_interest = [x for x in df_hotels.columns if x not in cols_to_forget]\n",
    "for cl in hotels_cols_interest:\n",
    "    k = df_hotels.select(cl).filter(F.col(cl).isNull() | F.isnan(cl)).count()\n",
    "    if k > 0:\n",
    "        print(f'Column {cl} with {k} nulls or NaN, out of {hotels_count} records ({k*100/hotels_count:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a208be-aa90-4478-937c-2ad9a7506f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ainda por ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d566d24-2350-4371-96e6-712d10c90a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_cols_interest.remove('orig_destination_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa1e98-0ca7-4eb9-a01f-56acbf51427c",
   "metadata": {},
   "source": [
    "### **Summary to figure out outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b6ff29a-646a-445f-a6b2-05eb332b895f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------\n",
      " summary                   | count               \n",
      " site_name                 | 37670293            \n",
      " posa_continent            | 37670293            \n",
      " user_location_country     | 37670293            \n",
      " user_location_region      | 37670293            \n",
      " user_location_city        | 37670293            \n",
      " orig_destination_distance | 24145292            \n",
      " user_id                   | 37670293            \n",
      " is_mobile                 | 37670293            \n",
      " is_package                | 37670293            \n",
      " channel                   | 37670293            \n",
      " srch_adults_cnt           | 37670293            \n",
      " srch_children_cnt         | 37670293            \n",
      " srch_rm_cnt               | 37670293            \n",
      " srch_destination_id       | 37670293            \n",
      " srch_destination_type_id  | 37670293            \n",
      " is_booking                | 37670293            \n",
      " cnt                       | 37670293            \n",
      " hotel_continent           | 37670293            \n",
      " hotel_country             | 37670293            \n",
      " hotel_market              | 37670293            \n",
      " hotel_cluster             | 37670293            \n",
      "-RECORD 1----------------------------------------\n",
      " summary                   | mean                \n",
      " site_name                 | 9.795271329585889   \n",
      " posa_continent            | 2.6804730188851997  \n",
      " user_location_country     | 86.10880194109454   \n",
      " user_location_region      | 308.4060117610447   \n",
      " user_location_city        | 27753.044729330883  \n",
      " orig_destination_distance | 1970.0900267207285  \n",
      " user_id                   | 604451.7531778422   \n",
      " is_mobile                 | 0.1349265056154461  \n",
      " is_package                | 0.24890422275186444 \n",
      " channel                   | 5.8707613981128315  \n",
      " srch_adults_cnt           | 2.0242958290767743  \n",
      " srch_children_cnt         | 0.3321221579030458  \n",
      " srch_rm_cnt               | 1.1126628083301608  \n",
      " srch_destination_id       | 14441.090543760836  \n",
      " srch_destination_type_id  | 2.5822799148389954  \n",
      " is_booking                | 0.07965674702875288 \n",
      " cnt                       | 1.4833839227106622  \n",
      " hotel_continent           | 3.1563047837190967  \n",
      " hotel_country             | 81.29685165974153   \n",
      " hotel_market              | 600.461883638654    \n",
      " hotel_cluster             | 49.80860501934509   \n",
      "-RECORD 2----------------------------------------\n",
      " summary                   | stddev              \n",
      " site_name                 | 11.9675435665128    \n",
      " posa_continent            | 0.7480393482506577  \n",
      " user_location_country     | 59.24310334783878   \n",
      " user_location_region      | 208.44374973856722  \n",
      " user_location_city        | 16782.553195680346  \n",
      " orig_destination_distance | 2232.4424303904275  \n",
      " user_id                   | 350617.4620408585   \n",
      " is_mobile                 | 0.34164505966916764 \n",
      " is_package                | 0.43237820899182056 \n",
      " channel                   | 3.71709455862911    \n",
      " srch_adults_cnt           | 0.9116678125665977  \n",
      " srch_children_cnt         | 0.7314980986397146  \n",
      " srch_rm_cnt               | 0.45911549963856946 \n",
      " srch_destination_id       | 11066.302332627309  \n",
      " srch_destination_type_id  | 2.153018959399955   \n",
      " is_booking                | 0.2707610600283715  \n",
      " cnt                       | 1.2197755786558424  \n",
      " hotel_continent           | 1.6231886782105662  \n",
      " hotel_country             | 56.171188062887815  \n",
      " hotel_market              | 511.73912727922396  \n",
      " hotel_cluster             | 28.915950805004293  \n",
      "-RECORD 3----------------------------------------\n",
      " summary                   | min                 \n",
      " site_name                 | 2                   \n",
      " posa_continent            | 0                   \n",
      " user_location_country     | 0                   \n",
      " user_location_region      | 0                   \n",
      " user_location_city        | 0                   \n",
      " orig_destination_distance | 0.0056              \n",
      " user_id                   | 0                   \n",
      " is_mobile                 | 0                   \n",
      " is_package                | 0                   \n",
      " channel                   | 0                   \n",
      " srch_adults_cnt           | 0                   \n",
      " srch_children_cnt         | 0                   \n",
      " srch_rm_cnt               | 0                   \n",
      " srch_destination_id       | 0                   \n",
      " srch_destination_type_id  | 0                   \n",
      " is_booking                | 0                   \n",
      " cnt                       | 1                   \n",
      " hotel_continent           | 0                   \n",
      " hotel_country             | 0                   \n",
      " hotel_market              | 0                   \n",
      " hotel_cluster             | 0                   \n",
      "-RECORD 4----------------------------------------\n",
      " summary                   | max                 \n",
      " site_name                 | 53                  \n",
      " posa_continent            | 4                   \n",
      " user_location_country     | 239                 \n",
      " user_location_region      | 1027                \n",
      " user_location_city        | 56508               \n",
      " orig_destination_distance | 12407.9022          \n",
      " user_id                   | 1198785             \n",
      " is_mobile                 | 1                   \n",
      " is_package                | 1                   \n",
      " channel                   | 10                  \n",
      " srch_adults_cnt           | 9                   \n",
      " srch_children_cnt         | 9                   \n",
      " srch_rm_cnt               | 8                   \n",
      " srch_destination_id       | 65107               \n",
      " srch_destination_type_id  | 9                   \n",
      " is_booking                | 1                   \n",
      " cnt                       | 269                 \n",
      " hotel_continent           | 6                   \n",
      " hotel_country             | 212                 \n",
      " hotel_market              | 2117                \n",
      " hotel_cluster             | 99                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hotels.describe(hotels_cols_interest).show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3d636-7a86-4ce3-ba7c-ab3f61bc839a",
   "metadata": {},
   "source": [
    "### **UNIQUENESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faabc16b-7463-4417-936e-38d559a82525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness in Hotels:\n",
      "Column site_name with 45 distinct values, out of 37670293 records (0.00%)\n",
      "Column posa_continent with 5 distinct values, out of 37670293 records (0.00%)\n",
      "Column user_location_country with 237 distinct values, out of 37670293 records (0.00%)\n",
      "Column user_location_region with 1008 distinct values, out of 37670293 records (0.00%)\n",
      "Column user_location_city with 50447 distinct values, out of 37670293 records (0.13%)\n",
      "Column orig_destination_distance with 8495290 distinct values, out of 37670293 records (22.55%)\n",
      "Column user_id with 1198786 distinct values, out of 37670293 records (3.18%)\n",
      "Column is_mobile with 2 distinct values, out of 37670293 records (0.00%)\n",
      "Column is_package with 2 distinct values, out of 37670293 records (0.00%)\n",
      "Column channel with 11 distinct values, out of 37670293 records (0.00%)\n",
      "Column srch_adults_cnt with 10 distinct values, out of 37670293 records (0.00%)\n",
      "Column srch_children_cnt with 10 distinct values, out of 37670293 records (0.00%)\n",
      "Column srch_rm_cnt with 9 distinct values, out of 37670293 records (0.00%)\n",
      "Column srch_destination_id with 59455 distinct values, out of 37670293 records (0.16%)\n",
      "Column srch_destination_type_id with 10 distinct values, out of 37670293 records (0.00%)\n",
      "Column is_booking with 2 distinct values, out of 37670293 records (0.00%)\n",
      "Column cnt with 104 distinct values, out of 37670293 records (0.00%)\n",
      "Column hotel_continent with 7 distinct values, out of 37670293 records (0.00%)\n",
      "Column hotel_country with 213 distinct values, out of 37670293 records (0.00%)\n",
      "Column hotel_market with 2118 distinct values, out of 37670293 records (0.01%)\n",
      "Column hotel_cluster with 100 distinct values, out of 37670293 records (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print('\\nUniqueness in Hotels:')\n",
    "for cl in hotels_cols_interest:\n",
    "    k = df_hotels.select(cl).distinct().count()\n",
    "    if k > 0:\n",
    "        print(f'Column {cl} with {k} distinct values, out of {hotels_count} records ({k*100/hotels_count:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc31664-266c-4907-ad5d-6124ba034eb1",
   "metadata": {},
   "source": [
    "### **Saving clean data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e68e18c-7d99-459f-84de-be2224ea0ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "('An error occurred while calling o375.sample',)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"python cell\", line 4, in <module>",
      "  File \"/opt/spark/python/pyspark/sql/dataframe.py\", line 1279, in sample\n    jdf = self._jdf.sample(*args)",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 190, in deco\n    return f(*a, **kw)",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 334, in get_return_value\n    raise Py4JError(",
      "Py4JError: An error occurred while calling o375.sample"
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "with_replacement = False\n",
    "fraction = 0.2          \n",
    "small_df_hotels = df_hotels.sample(withReplacement=with_replacement, \n",
    "                                               fraction=fraction, seed=seed)\n",
    "small_df_hotels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17782d9b-9361-468f-a754-b138a2e52c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df_hotels.write.mode(\"overwrite\").parquet(\"small-hotels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da1e91-168a-449a-ba7f-3a23c206c23f",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3289236-39fe-4db2-a216-f52e86837cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 19:53:02 WARN BlockManager: Block rdd_403_2 could not be removed as it was not found on disk or in memory\n",
      "23/03/22 19:53:02 WARN BlockManager: Block rdd_404_2 could not be removed as it was not found on disk or in memory\n",
      "23/03/22 19:53:02 WARN BlockManager: Block rdd_403_1 could not be removed as it was not found on disk or in memory\n",
      "23/03/22 19:53:02 WARN BlockManager: Block rdd_404_1 could not be removed as it was not found on disk or in memory\n",
      "23/03/22 19:53:02 ERROR Executor: Exception in task 2.0 in stage 201.0 (TID 1540)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3865)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:475)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.ensureSize(ArrayBuilder.scala:487)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:492)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable$$Lambda$20/0x000000084009b840.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofFloat.foreach(ArrayOps.scala:270)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:505)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1426)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "23/03/22 19:53:02 ERROR Executor: Exception in task 1.0 in stage 201.0 (TID 1539)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:343)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.ensureSize(ArrayBuilder.scala:355)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:360)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1429)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2292/0x0000000840fcb040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "23/03/22 19:53:02 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 201.0 (TID 1539),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:343)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.ensureSize(ArrayBuilder.scala:355)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:360)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1429)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2292/0x0000000840fcb040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "23/03/22 19:53:02 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 201.0 (TID 1540),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3865)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:475)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.ensureSize(ArrayBuilder.scala:487)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:492)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable$$Lambda$20/0x000000084009b840.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofFloat.foreach(ArrayOps.scala:270)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:505)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1426)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "23/03/22 19:53:02 WARN TaskSetManager: Lost task 2.0 in stage 201.0 (TID 1540) (192.168.89.130 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3865)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:475)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.ensureSize(ArrayBuilder.scala:487)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:492)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable$$Lambda$20/0x000000084009b840.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofFloat.foreach(ArrayOps.scala:270)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:505)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1426)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\n",
      "23/03/22 19:53:02 ERROR TaskSetManager: Task 2 in stage 201.0 failed 1 times; aborting job\n",
      "23/03/22 19:53:03 WARN BlockManager: Block rdd_403_3 could not be removed as it was not found on disk or in memory\n",
      "23/03/22 19:53:03 WARN BlockManager: Block rdd_404_3 could not be removed as it was not found on disk or in memory\n",
      "23/03/22 19:53:03 ERROR Executor: Exception in task 3.0 in stage 201.0 (TID 1541)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/03/22 19:53:03 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 201.0 (TID 1541),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/03/22 19:53:03 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 201.0 failed 1 times, most recent failure: Lost task 2.0 in stage 201.0 (TID 1540) (192.168.89.130 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3865)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:475)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.ensureSize(ArrayBuilder.scala:487)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:492)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable$$Lambda$20/0x000000084009b840.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofFloat.foreach(ArrayOps.scala:270)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:505)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1426)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:973)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3865)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:475)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.ensureSize(ArrayBuilder.scala:487)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:492)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable$$Lambda$20/0x000000084009b840.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofFloat.foreach(ArrayOps.scala:270)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:505)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:462)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1426)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\n",
      "23/03/22 19:53:03 WARN BlockManager: Putting block rdd_403_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/03/22 19:53:03 WARN BlockManager: Block rdd_403_4 could not be removed as it was not found on disk or in memory\n",
      "23/03/22 19:53:03 WARN BlockManager: Putting block rdd_404_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/03/22 19:53:03 WARN BlockManager: Block rdd_404_4 could not be removed as it was not found on disk or in memory\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "('An error occurred while calling o381.fit.\\n', JavaObject id=o433)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"python cell\", line 14, in <module>",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 205, in fit\n    return self._fit(dataset)",
      "  File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 383, in _fit\n    java_model = self._fit_java(dataset)",
      "  File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 380, in _fit_java\n    return self._java_obj.fit(dataset._jdf)",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 190, in deco\n    return f(*a, **kw)",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(",
      "Py4JJavaError: An error occurred while calling o381.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 201.0 failed 1 times, most recent failure: Lost task 2.0 in stage 201.0 (TID 1540) (192.168.89.130 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3865)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:475)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.ensureSize(ArrayBuilder.scala:487)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:492)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:462)\n\tat scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)\n\tat scala.collection.generic.Growable$$Lambda$20/0x000000084009b840.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofFloat.foreach(ArrayOps.scala:270)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:505)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:462)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1426)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:973)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3865)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:475)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.ensureSize(ArrayBuilder.scala:487)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:492)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$eq(ArrayBuilder.scala:462)\n\tat scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)\n\tat scala.collection.generic.Growable$$Lambda$20/0x000000084009b840.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofFloat.foreach(ArrayOps.scala:270)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:505)\n\tat scala.collection.mutable.ArrayBuilder$ofFloat.$plus$plus$eq(ArrayBuilder.scala:462)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1426)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1643)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4147/0x00000008415d9040.apply(Unknown Source)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1642)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4143/0x00000008415ec040.apply(Unknown Source)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4124/0x00000008415ea040.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessÃ¡rias\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Criando um vetor com os atributos que serÃ£o usados para fazer as recomendaÃ§Ãµes\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"srch_children_cnt\", \"is_mobile\", \"is_booking\", \"is_package\", \"user_location_country\"], outputCol=\"features\")\n",
    "df_hotels = vectorAssembler.transform(df_hotels)\n",
    "\n",
    "# Separando os dados em conjuntos de treinamento e teste\n",
    "(training, test) = df_hotels.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Configurando o modelo ALS (Alternating Least Squares) para fazer as recomendaÃ§Ãµes\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"hotel_cluster\", ratingCol=\"is_booking\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Fazendo as recomendaÃ§Ãµes para cada usuÃ¡rio com base em seus histÃ³ricos de reservas e outros atributos\n",
    "recommendations = model.recommendForAllUsers(10)\n",
    "\n",
    "# Mostrando as recomendaÃ§Ãµes para um usuÃ¡rio especÃ­fico\n",
    "recommendations.filter(recommendations.user_id == 1234).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0fb95-53a5-4866-8b3c-a51fffc60ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
