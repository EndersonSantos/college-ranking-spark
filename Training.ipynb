{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2693084-6718-4221-8044-28587d7cff80",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9138ddb-88fd-4cf2-97f1-1750ecff2d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/09 15:33:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/09 15:33:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "    # build our own SparkSession\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"BigData\")\n",
    "        .config(\"spark.sql.shuffle.partitions\",6)\n",
    "        .config(\"spark.sql.repl.eagereval.enabled\",True)\n",
    "        .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec88f023-703d-472f-9521-92a043c65f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotels = spark.read.parquet('small-hotels')\n",
    "#df_hotels = spark.read.parquet('whole-hotels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcaf2f0-bf55-4199-9e52-a38d5e102bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2227b75f-0165-4383-aa35-bac653d47e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " date_time             | 2014-09-02 01:05:54 \n",
      " site_name             | 2                   \n",
      " posa_continent        | 3                   \n",
      " user_location_country | 75                  \n",
      " user_location_region  | 144                 \n",
      " user_location_city    | 52467               \n",
      " user_id               | 1145960             \n",
      " mobile                | 0                   \n",
      " package               | 0                   \n",
      " channel               | 9                   \n",
      " srch_ci               | 2014-09-03 00:00:00 \n",
      " srch_co               | 2014-09-08 00:00:00 \n",
      " num_adults            | 2                   \n",
      " num_children          | 0                   \n",
      " num_room              | 1                   \n",
      " id_destination        | 46375               \n",
      " type_destination      | 1                   \n",
      " booking               | 0                   \n",
      " similar_srch          | 1                   \n",
      " hotel_continent       | 5                   \n",
      " hotel_country         | 147                 \n",
      " hotel_market          | 143                 \n",
      " hotel_cluster         | 30                  \n",
      " Id_hotel              | 301431475           \n",
      " num_nights            | 5                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hotels.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48507543-1272-4fde-aadf-82ec55426c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotel = df_hotels.select(\"num_nights\", \"similar_srch\", \"num_room\", \"num_adults\", \"num_children\", \"channel\", \"mobile\", \"booking\", \"package\", \"user_location_country\", \"user_location_city\", \"user_location_region\", \"Id_hotel\", \"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc97c4-916f-4e11-85fb-7c0c797cafbe",
   "metadata": {},
   "source": [
    "## Applying Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1dbe43-8ecd-4892-a400-2d8c62d41607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, when, col\n",
    "\n",
    "# Calcular a média de \"booking\" quando igual a 1\n",
    "booking_avg = df_hotels.select(avg(when(col(\"booking\") == 1, 1))).collect()[0][0]\n",
    "\n",
    "# Calcular a porcentagem de \"booking\" igual a 1\n",
    "booking_percentage = df_hotels.filter(col(\"booking\") == 1).count() / df_hotels.count()\n",
    "\n",
    "# Adicionar a porcentagem à coluna \"booking\"\n",
    "df_hotels = df_hotels.withColumn(\"booking\", col(\"booking\") + booking_percentage * booking_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d3878-0795-4697-8269-977f55e06521",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1530a366-aa0c-4992-92c0-69dcd06b308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------+------------------+--------------------+--------------------+-----------------+-------+---------------+\n",
      "|user_location_country| country_encoded|user_location_city|        city_encoded|user_location_region|   region_encoded|channel|channel_encoded|\n",
      "+---------------------+----------------+------------------+--------------------+--------------------+-----------------+-------+---------------+\n",
      "|                   75|(212,[58],[1.0])|             52467| (17894,[287],[1.0])|                 144|(857,[156],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                  218|(212,[40],[1.0])|             39100| (17894,[217],[1.0])|                  17|(857,[127],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                  218|(212,[40],[1.0])|             39100| (17894,[217],[1.0])|                  17|(857,[127],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|              4948| (17894,[150],[1.0])|                 220|  (857,[4],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|              4948| (17894,[150],[1.0])|                 220|  (857,[4],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|              4948| (17894,[150],[1.0])|                 220|  (857,[4],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|              4948| (17894,[150],[1.0])|                 220|  (857,[4],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                  205| (212,[1],[1.0])|             25315|   (17894,[3],[1.0])|                 354|  (857,[2],[1.0])|      2| (10,[3],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|             24503|(17894,[1137],[1.0])|                 258|  (857,[9],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   71|(212,[31],[1.0])|             22827|(17894,[5706],[1.0])|                   0| (857,[16],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|             21309|(17894,[4083],[1.0])|                 337| (857,[10],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                  205| (212,[1],[1.0])|             40095| (17894,[198],[1.0])|                 155|  (857,[7],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|              2086|   (17894,[8],[1.0])|                 220|  (857,[4],[1.0])|      7| (10,[7],[1.0])|\n",
      "|                   62|(212,[24],[1.0])|              7652|(17894,[17571],[1...|                  24|(857,[240],[1.0])|      1| (10,[2],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|             29563|(17894,[1084],[1.0])|                 442|  (857,[3],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|              2822| (17894,[581],[1.0])|                 442|  (857,[3],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|             21621| (17894,[858],[1.0])|                 442|  (857,[3],[1.0])|      1| (10,[2],[1.0])|\n",
      "|                  215| (212,[7],[1.0])|             13087| (17894,[199],[1.0])|                 817| (857,[94],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                   66| (212,[0],[1.0])|             32759| (17894,[332],[1.0])|                 314| (857,[26],[1.0])|      9| (10,[0],[1.0])|\n",
      "|                  133| (212,[8],[1.0])|             18770|(17894,[12294],[1...|                  31|(857,[117],[1.0])|      9| (10,[0],[1.0])|\n",
      "+---------------------+----------------+------------------+--------------------+--------------------+-----------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Creating StringIndexer objects for each categorical variable\n",
    "indexer_country = StringIndexer(inputCol=\"user_location_country\", outputCol=\"country_index\")\n",
    "indexer_city = StringIndexer(inputCol=\"user_location_city\", outputCol=\"city_index\")\n",
    "indexer_region = StringIndexer(inputCol=\"user_location_region\", outputCol=\"region_index\")\n",
    "indexer_channel = StringIndexer(inputCol=\"channel\", outputCol=\"channel_index\")\n",
    "\n",
    "# Fitting the StringIndexer objects on the data\n",
    "indexer_model_country = indexer_country.fit(df_hotel)\n",
    "indexer_model_city = indexer_city.fit(df_hotel)\n",
    "indexer_model_region = indexer_region.fit(df_hotel)\n",
    "indexer_model_channel = indexer_channel.fit(df_hotel)\n",
    "\n",
    "# Transforming the data using the StringIndexer objects\n",
    "df_hotel_indexed = indexer_model_country.transform(df_hotel)\n",
    "df_hotel_indexed = indexer_model_city.transform(df_hotel_indexed)\n",
    "df_hotel_indexed = indexer_model_region.transform(df_hotel_indexed)\n",
    "df_hotel_indexed = indexer_model_channel.transform(df_hotel_indexed)\n",
    "\n",
    "# Creating OneHotEncoder objects for each categorical variable\n",
    "encoder_country = OneHotEncoder(inputCol=\"country_index\", outputCol=\"country_encoded\")\n",
    "encoder_city = OneHotEncoder(inputCol=\"city_index\", outputCol=\"city_encoded\")\n",
    "encoder_region = OneHotEncoder(inputCol=\"region_index\", outputCol=\"region_encoded\")\n",
    "encoder_channel = OneHotEncoder(inputCol=\"channel_index\", outputCol=\"channel_encoded\")\n",
    "\n",
    "# Fitting the OneHotEncoder objects on the data\n",
    "encoder_model_country = encoder_country.fit(df_hotel_indexed)\n",
    "encoder_model_city = encoder_city.fit(df_hotel_indexed)\n",
    "encoder_model_region = encoder_region.fit(df_hotel_indexed)\n",
    "encoder_model_channel = encoder_channel.fit(df_hotel_indexed)\n",
    "\n",
    "# Transforming the data using the OneHotEncoder objects\n",
    "df_hotel_encoded = encoder_model_country.transform(df_hotel_indexed)\n",
    "df_hotel_encoded = encoder_model_city.transform(df_hotel_encoded)\n",
    "df_hotel_encoded = encoder_model_region.transform(df_hotel_encoded)\n",
    "df_hotel_encoded = encoder_model_channel.transform(df_hotel_encoded)\n",
    "\n",
    "# Displaying the encoded data\n",
    "df_hotel_encoded.select(\"user_location_country\", \"country_encoded\", \"user_location_city\", \"city_encoded\", \"user_location_region\", \"region_encoded\", \"channel\", \"channel_encoded\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e7cdd-3544-4996-bd4e-7564e409c3a2",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d860d888-ae3a-4f77-9967-9de7f3af5a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[5.0,1.0,1.0,2.0,...|[1.61125150707184...|\n",
      "|[3.0,3.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[5.0,1.0,1.0,2.0,...|[1.61125150707184...|\n",
      "|[5.0,1.0,1.0,1.0,...|[1.61125150707184...|\n",
      "|[1.0,1.0,1.0,2.0,...|[0.32225030141436...|\n",
      "|[2.0,1.0,1.0,2.0,...|[0.64450060282873...|\n",
      "|[2.0,1.0,1.0,2.0,...|[0.64450060282873...|\n",
      "|[3.0,1.0,1.0,4.0,...|[0.96675090424310...|\n",
      "|[2.0,2.0,2.0,2.0,...|[0.64450060282873...|\n",
      "|[1.0,1.0,1.0,2.0,...|[0.32225030141436...|\n",
      "|[2.0,2.0,1.0,1.0,...|[0.64450060282873...|\n",
      "|[3.0,1.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[3.0,1.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[3.0,4.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[1.0,1.0,1.0,2.0,...|[0.32225030141436...|\n",
      "|[2.0,2.0,2.0,2.0,...|[0.64450060282873...|\n",
      "|[9.0,3.0,2.0,3.0,...|[2.90025271272932...|\n",
      "|[2.0,2.0,1.0,1.0,...|[0.64450060282873...|\n",
      "|[5.0,1.0,1.0,4.0,...|[1.61125150707184...|\n",
      "|[2.0,1.0,1.0,2.0,...|[0.64450060282873...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "# Creating a VectorAssembler to assemble the features into a single vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"num_nights\", \"similar_srch\", \"num_room\", \"num_adults\", \"num_children\"], outputCol=\"features\")\n",
    "df_hotel = vectorAssembler.transform(df_hotel)\n",
    "\n",
    "# Creating a StandardScaler object to scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Fitting the StandardScaler on the data\n",
    "scaler_model = scaler.fit(df_hotel)\n",
    "\n",
    "# Transforming the data using the StandardScaler\n",
    "df_hotel_scaled = scaler_model.transform(df_hotel)\n",
    "\n",
    "# Displaying the scaled data\n",
    "df_hotel_scaled.select(\"features\", \"scaled_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f957b-b618-4564-8be1-b8873902aa22",
   "metadata": {},
   "source": [
    "## Doing Everything -> Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b410a8d8-5c6b-476c-a64e-da1cbcda2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  assembled_features|     scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[5.0,1.0,1.0,2.0,...|[1.61125150707184...|\n",
      "|[3.0,3.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[5.0,1.0,1.0,2.0,...|[1.61125150707184...|\n",
      "|[5.0,1.0,1.0,1.0,...|[1.61125150707184...|\n",
      "|[1.0,1.0,1.0,2.0,...|[0.32225030141436...|\n",
      "|[2.0,1.0,1.0,2.0,...|[0.64450060282873...|\n",
      "|[2.0,1.0,1.0,2.0,...|[0.64450060282873...|\n",
      "|[3.0,1.0,1.0,4.0,...|[0.96675090424310...|\n",
      "|[2.0,2.0,2.0,2.0,...|[0.64450060282873...|\n",
      "|[1.0,1.0,1.0,2.0,...|[0.32225030141436...|\n",
      "|[2.0,2.0,1.0,1.0,...|[0.64450060282873...|\n",
      "|[3.0,1.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[3.0,1.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[3.0,4.0,1.0,2.0,...|[0.96675090424310...|\n",
      "|[1.0,1.0,1.0,2.0,...|[0.32225030141436...|\n",
      "|[2.0,2.0,2.0,2.0,...|[0.64450060282873...|\n",
      "|[9.0,3.0,2.0,3.0,...|[2.90025271272932...|\n",
      "|[2.0,2.0,1.0,1.0,...|[0.64450060282873...|\n",
      "|[5.0,1.0,1.0,4.0,...|[1.61125150707184...|\n",
      "|[2.0,1.0,1.0,2.0,...|[0.64450060282873...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "# Creating StringIndexer objects for each categorical variable\n",
    "indexer_country = StringIndexer(inputCol=\"user_location_country\", outputCol=\"country_index\")\n",
    "indexer_city = StringIndexer(inputCol=\"user_location_city\", outputCol=\"city_index\")\n",
    "indexer_region = StringIndexer(inputCol=\"user_location_region\", outputCol=\"region_index\")\n",
    "indexer_channel = StringIndexer(inputCol=\"channel\", outputCol=\"channel_index\")\n",
    "\n",
    "# Creating OneHotEncoder objects for each categorical variable\n",
    "encoder_country = OneHotEncoder(inputCol=\"country_index\", outputCol=\"country_encoded\")\n",
    "encoder_city = OneHotEncoder(inputCol=\"city_index\", outputCol=\"city_encoded\")\n",
    "encoder_region = OneHotEncoder(inputCol=\"region_index\", outputCol=\"region_encoded\")\n",
    "encoder_channel = OneHotEncoder(inputCol=\"channel_index\", outputCol=\"channel_encoded\")\n",
    "\n",
    "# Creating a VectorAssembler to assemble the features into a single vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"num_nights\", \"similar_srch\", \"num_room\", \"num_adults\", \"num_children\"], outputCol=\"assembled_features\")\n",
    "df_hotel_assembled = vectorAssembler.transform(df_hotel_encoded)\n",
    "\n",
    "# Creating a StandardScaler object to scale the features\n",
    "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Fitting the StandardScaler on the data\n",
    "scaler_model = scaler.fit(df_hotel_assembled)\n",
    "\n",
    "# Transforming the data using the StandardScaler\n",
    "df_hotel_scaled = scaler_model.transform(df_hotel_assembled)\n",
    "\n",
    "# Displaying the scaled data\n",
    "df_hotel_scaled.select(\"assembled_features\", \"scaled_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c383b0-bac2-403f-b7bf-e53742631b90",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1674cc59-83ba-4d5c-ae85-1219712a8d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:11 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/04/09 15:33:11 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/04/09 15:33:11 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Criando um vetor com os atributos que serão usados para fazer as recomendações\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"num_nights\", \"similar_srch\", \"num_room\", \"num_adults\", \"num_children\", \"channel\", \"mobile\", \"booking\", \"package\", \"user_location_country\", \"user_location_city\", \"user_location_region\", \"scaled_features\"], outputCol=\"features\")\n",
    "df_hotels = vectorAssembler.transform(df_hotel_scaled)\n",
    "\n",
    "# Separando os dados em conjuntos de treinamento e teste\n",
    "#(training, test) = df_hotels.randomSplit([0.8, 0.2])\n",
    "\n",
    "training = df_hotels.sampleBy(\"booking\", fractions={0: 0.8, 1: 0.8}, seed=42)\n",
    "test = df_hotels.subtract(training)\n",
    "\n",
    "\n",
    "# Configurando o modelo ALS (Alternating Least Squares) para fazer as recomendações\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"Id_hotel\", ratingCol=\"booking\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "#recommendations = model.recommendForAllUsers(10)\n",
    "\n",
    "user_subset = df_hotels.filter(col(\"booking\") > 0).select('user_id')\n",
    "recommendations = model.recommendForUserSubset(user_subset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432972e2-7433-45f8-bd77-bc1d83752d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "-RECORD 0-------------------------------------\n",
      " num_nights            | 5                    \n",
      " similar_srch          | 1                    \n",
      " num_room              | 1                    \n",
      " num_adults            | 2                    \n",
      " num_children          | 0                    \n",
      " channel               | 9                    \n",
      " mobile                | 0                    \n",
      " booking               | 0                    \n",
      " package               | 0                    \n",
      " user_location_country | 75                   \n",
      " user_location_city    | 52467                \n",
      " user_location_region  | 144                  \n",
      " Id_hotel              | 301431475            \n",
      " user_id               | 1145960              \n",
      " country_index         | 58.0                 \n",
      " city_index            | 287.0                \n",
      " region_index          | 156.0                \n",
      " channel_index         | 0.0                  \n",
      " country_encoded       | (212,[58],[1.0])     \n",
      " city_encoded          | (17894,[287],[1.0])  \n",
      " region_encoded        | (857,[156],[1.0])    \n",
      " channel_encoded       | (10,[0],[1.0])       \n",
      " assembled_features    | [5.0,1.0,1.0,2.0,... \n",
      " scaled_features       | [1.61125150707184... \n",
      " features              | [5.0,1.0,1.0,2.0,... \n",
      "-RECORD 1-------------------------------------\n",
      " num_nights            | 3                    \n",
      " similar_srch          | 3                    \n",
      " num_room              | 1                    \n",
      " num_adults            | 2                    \n",
      " num_children          | 1                    \n",
      " channel               | 9                    \n",
      " mobile                | 0                    \n",
      " booking               | 0                    \n",
      " package               | 0                    \n",
      " user_location_country | 218                  \n",
      " user_location_city    | 39100                \n",
      " user_location_region  | 17                   \n",
      " Id_hotel              | 6212684              \n",
      " user_id               | 1145987              \n",
      " country_index         | 40.0                 \n",
      " city_index            | 217.0                \n",
      " region_index          | 127.0                \n",
      " channel_index         | 0.0                  \n",
      " country_encoded       | (212,[40],[1.0])     \n",
      " city_encoded          | (17894,[217],[1.0])  \n",
      " region_encoded        | (857,[127],[1.0])    \n",
      " channel_encoded       | (10,[0],[1.0])       \n",
      " assembled_features    | [3.0,3.0,1.0,2.0,... \n",
      " scaled_features       | [0.96675090424310... \n",
      " features              | [3.0,3.0,1.0,2.0,... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hotels.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f9b418-571d-4e8b-bad2-d0af33dd4073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:===================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|user_id|recommendations|\n",
      "+-------+---------------+\n",
      "+-------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:=====================================================>(99 + 1) / 100]\r"
     ]
    }
   ],
   "source": [
    "recommendations.filter(recommendations.user_id == 336709).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91d409-3da7-420b-bef5-30e4b26a86a6",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bcc08-57ea-4561-8171-8b71b29eb678",
   "metadata": {},
   "source": [
    "# Using the model with new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3665507a-7d49-4395-9616-a6d143204bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|user_id|recommendations|\n",
      "+-------+---------------+\n",
      "+-------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:==============================================>        (51 + 9) / 60]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# criar um novo DataFrame para um novo usuário\n",
    "new_user = Row(user_id=999, num_nights=3, similar_srch=2,num_room=1,num_adult=2, num_children=0,\n",
    "               channel=1, mobile=0, booking=0, package=1, user_location_country=50, user_location_city=1234,\n",
    "               user_location_region=123)\n",
    "\n",
    "new_user_df = spark.createDataFrame([new_user])\n",
    "\n",
    "# gerar recomendações para o novo usuário\n",
    "new_user_recs = model.recommendForUserSubset(new_user_df, 10)\n",
    "new_user_recs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773e41d-c512-45b0-a460-85d3691b54be",
   "metadata": {},
   "source": [
    "# Validar o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b95f7d-419d-4d65-8ca4-a33c371a6839",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "706f7d57-38fb-447c-8f1d-d7d090691f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:18 WARN DAGScheduler: Broadcasting large task binary with size 1535.5 KiB\n",
      "23/04/09 15:33:18 WARN DAGScheduler: Broadcasting large task binary with size 1556.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:21 WARN DAGScheduler: Broadcasting large task binary with size 1729.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:24 WARN DAGScheduler: Broadcasting large task binary with size 1605.5 KiB\n",
      "23/04/09 15:33:24 WARN DAGScheduler: Broadcasting large task binary with size 1670.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.21%\n",
      "23/04/09 15:33:25 WARN DAGScheduler: Broadcasting large task binary with size 1535.5 KiB\n",
      "23/04/09 15:33:25 WARN DAGScheduler: Broadcasting large task binary with size 1556.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:26 WARN DAGScheduler: Broadcasting large task binary with size 1729.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 304:=========>                                               (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1605.5 KiB\n",
      "23/04/09 15:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1670.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 86.75%\n",
      "23/04/09 15:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1535.5 KiB\n",
      "23/04/09 15:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1556.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:29 WARN DAGScheduler: Broadcasting large task binary with size 1729.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 392:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:31 WARN DAGScheduler: Broadcasting large task binary with size 1605.5 KiB\n",
      "23/04/09 15:33:31 WARN DAGScheduler: Broadcasting large task binary with size 1670.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 85.21%\n",
      "23/04/09 15:33:31 WARN DAGScheduler: Broadcasting large task binary with size 1535.5 KiB\n",
      "23/04/09 15:33:31 WARN DAGScheduler: Broadcasting large task binary with size 1556.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:32 WARN DAGScheduler: Broadcasting large task binary with size 1729.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 480:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:33:33 WARN DAGScheduler: Broadcasting large task binary with size 1605.5 KiB\n",
      "23/04/09 15:33:33 WARN DAGScheduler: Broadcasting large task binary with size 1670.1 KiB\n",
      "F1: 85.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions = predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n",
    "# Avalia as previsões do modelo nos dados de teste\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"booking\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"booking\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print(\"Precision: {:.2%}\".format(precision))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"booking\", metricName=\"weightedRecall\")\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print(\"Recall: {:.2%}\".format(recall))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"booking\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"F1: {:.2%}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a7f5a3-e426-49a4-9019-d503f3ad2dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8521256931608133\n",
      "Precision:  0.8674667449198459\n",
      "Recall:  0.8521256931608133\n",
      "f1_score 0.8597277877470005\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"f1_score\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b01b84-27f0-4eb4-a37e-55349a050912",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e54fb-d7cc-443d-b9d3-78c07c1f7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pyspark.serializers import PickleSerializer\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
